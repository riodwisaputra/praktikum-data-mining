{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN7q8rVxyAKHscxaarJg3m9"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["**Nama: Rio Dwi Saputra**\n","\n","**Nim: G.231.22.0074**"],"metadata":{"id":"kwzkPOHvwWHc"}},{"cell_type":"markdown","source":["**Supervised Learning**\n","\n","(neuroevolution)"],"metadata":{"id":"pbgeSyZTwjGJ"}},{"cell_type":"code","execution_count":13,"metadata":{"id":"Of-sglLnv0At","executionInfo":{"status":"ok","timestamp":1719946654343,"user_tz":-420,"elapsed":1159,"user":{"displayName":"Rio Saputra","userId":"15618685712198888100"}}},"outputs":[],"source":["from __future__ import print_function, division\n","import numpy as np\n","import copy\n","\n","class Neuroevolution():\n","    \"\"\" Evolutionary optimization of Neural Networks.\n","\n","    Parameters:\n","    -----------\n","    n_individuals: int\n","        The number of neural networks that are allowed in the population at a time.\n","    mutation_rate: float\n","        The probability that a weight will be mutated.\n","    model_builder: method\n","        A method which returns a user specified NeuralNetwork instance.\n","    \"\"\"\n","    def __init__(self, population_size, mutation_rate, model_builder):\n","        self.population_size = population_size\n","        self.mutation_rate = mutation_rate\n","        self.model_builder = model_builder\n","\n","    def _build_model(self, id):\n","        \"\"\" Returns a new individual \"\"\"\n","        model = self.model_builder(n_inputs=self.X.shape[1], n_outputs=self.y.shape[1])\n","        model.id = id\n","        model.fitness = 0\n","        model.accuracy = 0\n","\n","        return model\n","\n","    def _initialize_population(self):\n","        \"\"\" Initialization of the neural networks forming the population\"\"\"\n","        self.population = []\n","        for _ in range(self.population_size):\n","            model = self._build_model(id=np.random.randint(1000))\n","            self.population.append(model)\n","\n","    def _mutate(self, individual, var=1):\n","        \"\"\" Add zero mean gaussian noise to the layer weights with probability mutation_rate \"\"\"\n","        for layer in individual.layers:\n","            if hasattr(layer, 'W'):\n","                # Mutation of weight with probability self.mutation_rate\n","                mutation_mask = np.random.binomial(1, p=self.mutation_rate, size=layer.W.shape)\n","                layer.W += np.random.normal(loc=0, scale=var, size=layer.W.shape) * mutation_mask\n","                mutation_mask = np.random.binomial(1, p=self.mutation_rate, size=layer.w0.shape)\n","                layer.w0 += np.random.normal(loc=0, scale=var, size=layer.w0.shape) * mutation_mask\n","\n","        return individual\n","\n","    def _inherit_weights(self, child, parent):\n","        \"\"\" Copies the weights from parent to child \"\"\"\n","        for i in range(len(child.layers)):\n","            if hasattr(child.layers[i], 'W'):\n","                # The child inherits both weights W and bias weights w0\n","                child.layers[i].W = parent.layers[i].W.copy()\n","                child.layers[i].w0 = parent.layers[i].w0.copy()\n","\n","    def _crossover(self, parent1, parent2):\n","        \"\"\" Performs crossover between the neurons in parent1 and parent2 to form offspring \"\"\"\n","        child1 = self._build_model(id=parent1.id+1)\n","        self._inherit_weights(child1, parent1)\n","        child2 = self._build_model(id=parent2.id+1)\n","        self._inherit_weights(child2, parent2)\n","\n","        # Perform crossover\n","        for i in range(len(child1.layers)):\n","            if hasattr(child1.layers[i], 'W'):\n","                n_neurons = child1.layers[i].W.shape[1]\n","                # Perform crossover between the individuals' neuron weights\n","                cutoff = np.random.randint(0, n_neurons)\n","                child1.layers[i].W[:, cutoff:] = parent2.layers[i].W[:, cutoff:].copy()\n","                child1.layers[i].w0[:, cutoff:] = parent2.layers[i].w0[:, cutoff:].copy()\n","                child2.layers[i].W[:, cutoff:] = parent1.layers[i].W[:, cutoff:].copy()\n","                child2.layers[i].w0[:, cutoff:] = parent1.layers[i].w0[:, cutoff:].copy()\n","\n","        return child1, child2\n","\n","    def _calculate_fitness(self):\n","        \"\"\" Evaluate the NNs on the test set to get fitness scores \"\"\"\n","        for individual in self.population:\n","            loss, acc = individual.test_on_batch(self.X, self.y)\n","            individual.fitness = 1 / (loss + 1e-8)\n","            individual.accuracy = acc\n","\n","    def evolve(self, X, y, n_generations):\n","        \"\"\" Will evolve the population for n_generations based on dataset X and labels y\"\"\"\n","        self.X, self.y = X, y\n","\n","        self._initialize_population()\n","\n","        # The 40% highest fittest individuals will be selected for the next generation\n","        n_winners = int(self.population_size * 0.4)\n","        # The fittest 60% of the population will be selected as parents to form offspring\n","        n_parents = self.population_size - n_winners\n","\n","        for epoch in range(n_generations):\n","            # Determine the fitness of the individuals in the population\n","            self._calculate_fitness()\n","\n","            # Sort population by fitness\n","            sorted_i = np.argsort([model.fitness for model in self.population])[::-1]\n","            self.population = [self.population[i] for i in sorted_i]\n","\n","            # Get the individual with the highest fitness\n","            fittest_individual = self.population[0]\n","            print (\"[%d Best Individual - Fitness: %.5f, Accuracy: %.1f%%]\" % (epoch,\n","                                                                        fittest_individual.fitness,\n","                                                                        float(100*fittest_individual.accuracy)))\n","            # The 'winners' are selected for the next generation\n","            next_population = [self.population[i] for i in range(n_winners)]\n","\n","            total_fitness = np.sum([model.fitness for model in self.population])\n","            # The probability that a individual will be selected as a parent is proportionate to its fitness\n","            parent_probabilities = [model.fitness / total_fitness for model in self.population]\n","            # Select parents according to probabilities (without replacement to preserve diversity)\n","            parents = np.random.choice(self.population, size=n_parents, p=parent_probabilities, replace=False)\n","            for i in np.arange(0, len(parents), 2):\n","                # Perform crossover to produce offspring\n","                child1, child2 = self._crossover(parents[i], parents[i+1])\n","                # Save mutated offspring for next population\n","                next_population += [self._mutate(child1), self._mutate(child2)]\n","\n","            self.population = next_population\n","\n","        return fittest_individual"]},{"cell_type":"markdown","source":["**unsupervised learning**\n","\n","(apriori)"],"metadata":{"id":"mWtj5OZI0W62"}},{"cell_type":"code","source":["from __future__ import division, print_function\n","import numpy as np\n","import itertools\n","\n","\n","class Rule():\n","    def __init__(self, antecedent, concequent, confidence, support):\n","        self.antecedent = antecedent\n","        self.concequent = concequent\n","        self.confidence = confidence\n","        self.support = support\n","\n","\n","class Apriori():\n","    \"\"\"A method for determining frequent itemsets in a transactional database and\n","    also for generating rules for those itemsets.\n","\n","    Parameters:\n","    -----------\n","    min_sup: float\n","        The minimum fraction of transactions an itemets needs to\n","        occur in to be deemed frequent\n","    min_conf: float:\n","        The minimum fraction of times the antecedent needs to imply\n","        the concequent to justify rule\n","    \"\"\"\n","    def __init__(self, min_sup=0.3, min_conf=0.81):\n","\n","        self.min_sup = min_sup\n","        self.min_conf = min_conf\n","        self.freq_itemsets = None       # List of freqeuent itemsets\n","        self.transactions = None        # List of transactions\n","\n","    def _calculate_support(self, itemset):\n","        count = 0\n","        for transaction in self.transactions:\n","            if self._transaction_contains_items(transaction, itemset):\n","                count += 1\n","        support = count / len(self.transactions)\n","        return support\n","\n","\n","    def _get_frequent_itemsets(self, candidates):\n","        \"\"\" Prunes the candidates that are not frequent => returns list with\n","        only frequent itemsets \"\"\"\n","        frequent = []\n","        # Find frequent items\n","        for itemset in candidates:\n","            support = self._calculate_support(itemset)\n","            if support >= self.min_sup:\n","                frequent.append(itemset)\n","        return frequent\n","\n","\n","    def _has_infrequent_itemsets(self, candidate):\n","        \"\"\" True or false depending on the candidate has any\n","        subset with size k - 1 that is not in the frequent itemset \"\"\"\n","        k = len(candidate)\n","        # Find all combinations of size k-1 in candidate\n","        # E.g [1,2,3] => [[1,2],[1,3],[2,3]]\n","        subsets = list(itertools.combinations(candidate, k - 1))\n","        for t in subsets:\n","            # t - is tuple. If size == 1 get the element\n","            subset = list(t) if len(t) > 1 else t[0]\n","            if not subset in self.freq_itemsets[-1]:\n","                return True\n","        return False\n","\n","\n","    def _generate_candidates(self, freq_itemset):\n","        \"\"\" Joins the elements in the frequent itemset and prunes\n","        resulting sets if they contain subsets that have been determined\n","        to be infrequent. \"\"\"\n","        candidates = []\n","        for itemset1 in freq_itemset:\n","            for itemset2 in freq_itemset:\n","                # Valid if every element but the last are the same\n","                # and the last element in itemset1 is smaller than the last\n","                # in itemset2\n","                valid = False\n","                single_item = isinstance(itemset1, int)\n","                if single_item and itemset1 < itemset2:\n","                    valid = True\n","                elif not single_item and np.array_equal(itemset1[:-1], itemset2[:-1]) and itemset1[-1] < itemset2[-1]:\n","                    valid = True\n","\n","                if valid:\n","                    # JOIN: Add the last element in itemset2 to itemset1 to\n","                    # create a new candidate\n","                    if single_item:\n","                        candidate = [itemset1, itemset2]\n","                    else:\n","                        candidate = itemset1 + [itemset2[-1]]\n","                    # PRUNE: Check if any subset of candidate have been determined\n","                    # to be infrequent\n","                    infrequent = self._has_infrequent_itemsets(candidate)\n","                    if not infrequent:\n","                        candidates.append(candidate)\n","        return candidates\n","\n","\n","    def _transaction_contains_items(self, transaction, items):\n","        \"\"\" True or false depending on each item in the itemset is\n","        in the transaction \"\"\"\n","        # If items is in fact only one item\n","        if isinstance(items, int):\n","            return items in transaction\n","        # Iterate through list of items and make sure that\n","        # all items are in the transaction\n","        for item in items:\n","            if not item in transaction:\n","                return False\n","        return True\n","\n","    def find_frequent_itemsets(self, transactions):\n","        \"\"\" Returns the set of frequent itemsets in the list of transactions \"\"\"\n","        self.transactions = transactions\n","        # Get all unique items in the transactions\n","        unique_items = set(item for transaction in self.transactions for item in transaction)\n","        # Get the frequent items\n","        self.freq_itemsets = [self._get_frequent_itemsets(unique_items)]\n","        while(True):\n","            # Generate new candidates from last added frequent itemsets\n","            candidates = self._generate_candidates(self.freq_itemsets[-1])\n","            # Get the frequent itemsets among those candidates\n","            frequent_itemsets = self._get_frequent_itemsets(candidates)\n","\n","            # If there are no frequent itemsets we're done\n","            if not frequent_itemsets:\n","                break\n","\n","            # Add them to the total list of frequent itemsets and start over\n","            self.freq_itemsets.append(frequent_itemsets)\n","\n","        # Flatten the array and return every frequent itemset\n","        frequent_itemsets = [\n","            itemset for sublist in self.freq_itemsets for itemset in sublist]\n","        return frequent_itemsets\n","\n","\n","    def _rules_from_itemset(self, initial_itemset, itemset):\n","        \"\"\" Recursive function which returns the rules where confidence >= min_confidence\n","        Starts with large itemset and recursively explores rules for subsets \"\"\"\n","        rules = []\n","        k = len(itemset)\n","        # Get all combinations of sub-itemsets of size k - 1 from itemset\n","        # E.g [1,2,3] => [[1,2],[1,3],[2,3]]\n","        subsets = list(itertools.combinations(itemset, k - 1))\n","        support = self._calculate_support(initial_itemset)\n","        for antecedent in subsets:\n","            # itertools.combinations returns tuples => convert to list\n","            antecedent = list(antecedent)\n","            antecedent_support = self._calculate_support(antecedent)\n","            # Calculate the confidence as sup(A and B) / sup(B), if antecedent\n","            # is B in an itemset of A and B\n","            confidence = float(\"{0:.2f}\".format(support / antecedent_support))\n","            if confidence >= self.min_conf:\n","                # The concequent is the initial_itemset except for antecedent\n","                concequent = [itemset for itemset in initial_itemset if not itemset in antecedent]\n","                # If single item => get item\n","                if len(antecedent) == 1:\n","                    antecedent = antecedent[0]\n","                if len(concequent) == 1:\n","                    concequent = concequent[0]\n","                # Create new rule\n","                rule = Rule(\n","                        antecedent=antecedent,\n","                        concequent=concequent,\n","                        confidence=confidence,\n","                        support=support)\n","                rules.append(rule)\n","\n","                # If there are subsets that could result in rules\n","                # recursively add rules from subsets\n","                if k - 1 > 1:\n","                    rules += self._rules_from_itemset(initial_itemset, antecedent)\n","        return rules\n","\n","    def generate_rules(self, transactions):\n","        self.transactions = transactions\n","        frequent_itemsets = self.find_frequent_itemsets(transactions)\n","        # Only consider itemsets of size >= 2 items\n","        frequent_itemsets = [itemset for itemset in frequent_itemsets if not isinstance(\n","                itemset, int)]\n","        rules = []\n","        for itemset in frequent_itemsets:\n","            rules += self._rules_from_itemset(itemset, itemset)\n","        # Remove empty values\n","        return rules"],"metadata":{"id":"5UhIrnMr0cn5","executionInfo":{"status":"ok","timestamp":1719946829837,"user_tz":-420,"elapsed":477,"user":{"displayName":"Rio Saputra","userId":"15618685712198888100"}}},"execution_count":14,"outputs":[]}]}